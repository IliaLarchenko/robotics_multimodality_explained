{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodality In Robotics\n",
    "## Predicting a continuous action when multiple correct actions exist\n",
    "\n",
    "This notebook explores different approaches to handling multimodality in robotics.\n",
    "It is an addition to the video below. Please watch the video to understand the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"6oZe_tKE3YA\", width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github repo: https://github.com/IliaLarchenko/robotics_multimodality_explained\n",
    "\n",
    "Kaggle notebook: https://www.kaggle.com/code/ilialar/multimodality-explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# --- Data Generation ---\n",
    "NUM_EXPERT_SAMPLES = 10000\n",
    "N_TEST_SAMPLES = 1000\n",
    "\n",
    "DEFAULT_DATA_PARAMS = {\n",
    "    's_min': -1.2,\n",
    "    's_max': 1.2,\n",
    "    'safe_boundary': 1.0,\n",
    "    'target_mean_delta': 0.2,\n",
    "    'lognorm_sigma': 1.0\n",
    "}\n",
    "SAFE_BOUNDARY = DEFAULT_DATA_PARAMS['safe_boundary']\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model training utils\n",
    "\n",
    "HIDDEN_DIMS = (32, 32)\n",
    "OUTPUT_DIM = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" A simple MLP with configurable layers \"\"\"\n",
    "    def __init__(self, input_dim, output_dim=OUTPUT_DIM, hidden_dims=HIDDEN_DIMS):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "            last_dim = hidden_dim\n",
    "        layers.append(nn.Linear(last_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item() * batch_X.size(0)\n",
    "    epoch_train_loss /= len(train_loader.dataset)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(s_pred_input=None, x_pred_output=None, test_data=None, title=\"Data/Model Predictions\", model_label=\"Predicted\"):\n",
    "    \"\"\" \n",
    "    Plots:\n",
    "    1. Scatter plot of predictions (s vs x) overlaid with test data (if provided).\n",
    "       If predictions are None, only plots test data.\n",
    "    2. Overall density distribution comparison (predicted x vs test x).\n",
    "       If predictions are None, only plots test data distribution.\n",
    "    3. Conditional density distribution for s < 0 (if test_data provided).\n",
    "    4. Conditional density distribution for s > 0 (if test_data provided).\n",
    "    \n",
    "    Args:\n",
    "        s_pred_input: The input s values used for prediction (1D array) or None.\n",
    "        x_pred_output: The corresponding predicted x values (1D array) or None.\n",
    "        test_data: DataFrame with 's' and 'x' columns for ground truth/data points.\n",
    "        title: Base title for the plots.\n",
    "        model_label: Label for the predicted points/distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Fixed plotting params---\n",
    "    PLOT_FIG_SIZE_WIDE = (12, 5)\n",
    "    PLOT_ALPHA = 0.3\n",
    "    PLOT_POINT_SIZE = 10\n",
    "\n",
    "    if test_data is None and s_pred_input is None:\n",
    "        print(\"Warning: No data provided to plot_predictions.\")\n",
    "        return\n",
    "        \n",
    "    # Check if we are plotting predictions or just data\n",
    "    plot_preds = s_pred_input is not None and x_pred_output is not None\n",
    "    if plot_preds:\n",
    "        s_pred_input = np.asarray(s_pred_input).flatten()\n",
    "        x_pred_output = np.asarray(x_pred_output).flatten()\n",
    "        if s_pred_input.shape != x_pred_output.shape:\n",
    "            raise ValueError(\"s_pred_input and x_pred_output must have the same shape.\")\n",
    "    else: # Adjust title if only plotting data\n",
    "        title = title.replace(\"Model Predictions\", \"Data\")\n",
    "        model_label = \"\" # No model label needed\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(PLOT_FIG_SIZE_WIDE[0] * 1.5, PLOT_FIG_SIZE_WIDE[1] * 1.8))\n",
    "    axes = axes.flatten()\n",
    "    ax1, ax2, ax3, ax4 = axes[0], axes[1], axes[2], axes[3]\n",
    "    \n",
    "    bw_adjustment = 0.1\n",
    "\n",
    "    # --- Determine overall y-limits from all available valid data ---\n",
    "    all_y_data = []\n",
    "    if test_data is not None:\n",
    "        all_y_data.append(test_data['x'])\n",
    "    if plot_preds:\n",
    "        all_y_data.append(x_pred_output)\n",
    "    \n",
    "    ymin, ymax = (-SAFE_BOUNDARY * 1.2, SAFE_BOUNDARY * 1.2) # Default limits\n",
    "    if all_y_data:\n",
    "        valid_y = np.concatenate(all_y_data)\n",
    "        valid_y = valid_y[~np.isnan(valid_y)]\n",
    "        if len(valid_y) > 0:\n",
    "            ymin = min(np.min(valid_y), -SAFE_BOUNDARY) - 0.2\n",
    "            ymax = max(np.max(valid_y), SAFE_BOUNDARY) + 0.2\n",
    "\n",
    "    # --- Subplot 1: Scatter Plot (s vs x) ---\n",
    "    if test_data is not None:\n",
    "        ax1.scatter(test_data['s'], test_data['x'], alpha=PLOT_ALPHA, \n",
    "                    s=PLOT_POINT_SIZE, label='Data Points', color='blue') # Changed label\n",
    "    if plot_preds:\n",
    "        ax1.scatter(s_pred_input, x_pred_output, alpha=PLOT_ALPHA + 0.1, s=PLOT_POINT_SIZE,\n",
    "                    label=model_label, color='orange') \n",
    "    ax1.axhline(-SAFE_BOUNDARY, color='red', linestyle='--', label=f'Boundary (+/- {SAFE_BOUNDARY:.1f})')\n",
    "    ax1.axhline(SAFE_BOUNDARY, color='red', linestyle='--')\n",
    "    ax1.set_title(f'{title} - Scatter Plot')\n",
    "    ax1.set_xlabel('State(s)')\n",
    "    ax1.set_ylabel('Action (x)')\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(ymin, ymax)\n",
    "\n",
    "    # --- Subplot 2: Overall Density Plot (Distribution of x) ---\n",
    "    if test_data is not None:\n",
    "        sns.kdeplot(test_data['x'], ax=ax2, color='blue', label='Data Distribution', \n",
    "                    fill=True, alpha=PLOT_ALPHA, bw_adjust=bw_adjustment) # Changed label\n",
    "    if plot_preds:\n",
    "        valid_x_pred_output = x_pred_output[~np.isnan(x_pred_output)]\n",
    "        if len(valid_x_pred_output) > 0:\n",
    "            sns.kdeplot(valid_x_pred_output, ax=ax2, color='orange', label=model_label, \n",
    "                        fill=True, alpha=PLOT_ALPHA + 0.1, bw_adjust=bw_adjustment)\n",
    "        else:\n",
    "             ax2.text(0.5, 0.5, 'No valid preds', ha='center', va='center', transform=ax2.transAxes)\n",
    "    ax2.set_title(f'{title} - Overall Output Dist.')\n",
    "    ax2.set_xlabel('Action (x)')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.grid(True, linestyle=':')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlim(ymin, ymax) # Use consistent limits\n",
    "\n",
    "    # Conditional plots only make sense if we have test_data with s\n",
    "    if test_data is not None:\n",
    "        # Use test_data['s'] for conditioning, even when plotting predictions\n",
    "        s_for_conditioning = test_data['s'].values\n",
    "        \n",
    "        # --- Subplot 3: Conditional Density Plot (s < 0) ---\n",
    "        neg_s_indices = np.where(s_for_conditioning < 0)[0]\n",
    "        test_data_neg = test_data.iloc[neg_s_indices]\n",
    "        pred_output_neg = x_pred_output[neg_s_indices] if plot_preds else None\n",
    "        \n",
    "        if not test_data_neg.empty:\n",
    "            sns.kdeplot(test_data_neg['x'], ax=ax3, color='blue', label='Data (s<0)', \n",
    "                        fill=True, alpha=PLOT_ALPHA, bw_adjust=bw_adjustment)\n",
    "        if plot_preds:\n",
    "            valid_pred_neg = pred_output_neg[~np.isnan(pred_output_neg)]\n",
    "            if len(valid_pred_neg) > 0:\n",
    "                sns.kdeplot(valid_pred_neg, ax=ax3, color='orange', label=f'{model_label} (s<0)', \n",
    "                            fill=True, alpha=PLOT_ALPHA + 0.1, bw_adjust=bw_adjustment)\n",
    "            else:\n",
    "                 # Only add text if there were supposed to be predictions\n",
    "                 ax3.text(0.5, 0.5, 'No valid preds for s<0', ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.set_title(f'{title} - Output Dist. (s < 0)')\n",
    "        ax3.set_xlabel('Action (x)')\n",
    "        ax3.set_ylabel('Density')\n",
    "        ax3.grid(True, linestyle=':')\n",
    "        ax3.legend()\n",
    "        ax3.set_xlim(ymin, ymax) \n",
    "\n",
    "        # --- Subplot 4: Conditional Density Plot (s > 0) ---\n",
    "        pos_s_indices = np.where(s_for_conditioning > 0)[0]\n",
    "        test_data_pos = test_data.iloc[pos_s_indices]\n",
    "        pred_output_pos = x_pred_output[pos_s_indices] if plot_preds else None\n",
    "\n",
    "        if not test_data_pos.empty:\n",
    "            sns.kdeplot(test_data_pos['x'], ax=ax4, color='blue', label='Data (s>0)', \n",
    "                        fill=True, alpha=PLOT_ALPHA, bw_adjust=bw_adjustment)\n",
    "        if plot_preds:\n",
    "            valid_pred_pos = pred_output_pos[~np.isnan(pred_output_pos)]\n",
    "            if len(valid_pred_pos) > 0:\n",
    "                sns.kdeplot(valid_pred_pos, ax=ax4, color='orange', label=f'{model_label} (s>0)', \n",
    "                            fill=True, alpha=PLOT_ALPHA + 0.1, bw_adjust=bw_adjustment)\n",
    "            else:\n",
    "                 # Only add text if there were supposed to be predictions\n",
    "                 ax4.text(0.5, 0.5, 'No valid preds for s>0', ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title(f'{title} - Output Dist. (s > 0)')\n",
    "        ax4.set_xlabel('Action (x)')\n",
    "        ax4.set_ylabel('Density')\n",
    "        ax4.grid(True, linestyle=':')\n",
    "        ax4.legend()\n",
    "        ax4.set_xlim(ymin, ymax)\n",
    "    else: # Hide axes 3 and 4 if no test_data to condition on\n",
    "        ax3.axis('off')\n",
    "        ax4.axis('off')\n",
    "\n",
    "    # --- Final Touches ---\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration of the diffusion process\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def add_diffusion_noise(x0, b_t):\n",
    "    noise = np.random.randn(*x0.shape)\n",
    "    x_t = np.sqrt(1- b_t) * x0 + np.sqrt(b_t) * noise\n",
    "    return np.clip(x_t, 0, 1)\n",
    "\n",
    "def get_noisy_steps(img_path, T):\n",
    "# Load the original image\n",
    "    original_img = Image.open(img_path).convert(\"RGB\")\n",
    "    original_np = np.array(original_img) / 255.0  # Normalize\n",
    "\n",
    "    b_t = np.linspace(0.1, 0.8, T)  # noise schedule\n",
    "    noisy_steps = [original_np]\n",
    "    for b in b_t:\n",
    "        noisy_steps.append(add_diffusion_noise(noisy_steps[-1], b))\n",
    "\n",
    "    return noisy_steps  \n",
    "\n",
    "def plot_noisy_steps(noisy_steps, reverse=False, title=\"Denoising diffusion model training\"):\n",
    "    T = len(noisy_steps) - 1\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, T + 1, figsize=(18, 5))\n",
    "    titles = [\"Original image\"] + [\"\"] * (len(noisy_steps) - 2) +  [\"Pure noise\"]\n",
    "    if reverse:\n",
    "        titles = titles[::-1]\n",
    "        noisy_steps = list(noisy_steps[::-1])\n",
    "\n",
    "    for i, (ax, img) in enumerate(zip(axes, noisy_steps)):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(titles[i], fontsize=13)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    arrow = \"→\" if reverse else \"←\"\n",
    "    for i in range(1, len(axes)):\n",
    "        fig.text(0.20 * i, 0.07, arrow, fontsize=20, ha='center')\n",
    "\n",
    "    # Add text under the arrows\n",
    "    fig.text(0.5, 0.01, title, fontsize=13, ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolution_distributions(intermediate_steps):\n",
    "    \"\"\" Plots distributions at different steps of a generative process. Uses config. \"\"\"\n",
    "\n",
    "    num_steps_to_plot = len(intermediate_steps)\n",
    "    cols = min(5, num_steps_to_plot) # Adjust columns for potentially more steps\n",
    "    rows = (num_steps_to_plot + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3.5, rows * 3), sharex=True, sharey=False)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    all_xt_values = np.concatenate(intermediate_steps)\n",
    "\n",
    "    x_min, x_max = np.percentile(all_xt_values, [1, 99])\n",
    "    x_padding = max((x_max - x_min) * 0.1, 0.1)\n",
    "    x_lims = (x_min - x_padding, x_max + x_padding)\n",
    "\n",
    "    # Plot steps in logical order (t=0 to T or T down to 0 depending on process)\n",
    "    for i, step_data in enumerate(intermediate_steps):\n",
    "        ax = axes[i]\n",
    "        sns.histplot(step_data, ax=ax, bins=50, stat='density')\n",
    "        step_label = i\n",
    "\n",
    "        ax.set_title(f'Step {step_label}/{num_steps_to_plot - 1}')\n",
    "        ax.set_xlabel('x value')\n",
    "        ax.set_xlim(x_lims)\n",
    "        ax.grid(True, linestyle=':')\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    fig.suptitle(f'Step by step change in the data distribution')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of multimodal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, t, lognorm, laplace, triang, beta, gamma, logistic, weibull_min\n",
    "\n",
    "# Common x-axis\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "def plot_dist(ax, y, title):\n",
    "    ax.plot(x, y, lw=5, color='orange')\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(labelsize=10)\n",
    "\n",
    "def plot_all(dists, title):\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(15, 10))\n",
    "    for ax, (name, y) in zip(axs.flat, dists):\n",
    "        plot_dist(ax, y, name)\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# === Unimodal distributions ===\n",
    "unimodal = [\n",
    "    (\"Normal (mean=0, std=1)\", norm.pdf(x, 0, 1)),\n",
    "    (\"Student's t (df=3)\", t.pdf(x, df=3)),\n",
    "    (\"Log-Normal\", lognorm.pdf(x, s=0.5, scale=np.exp(0))),\n",
    "    (\"Laplace\", laplace.pdf(x, 0, 1)),\n",
    "    (\"Triangular\", triang.pdf(x, c=0.5, loc=-3, scale=6)),\n",
    "    (\"Beta (a=2, b=5) scaled\", beta.pdf((x + 3)/6, a=2, b=5) * (1/6)),\n",
    "    (\"Gamma (a=2)\", gamma.pdf(x + 5, a=2)),\n",
    "    (\"Logistic\", logistic.pdf(x)),\n",
    "    (\"Weibull (c=2)\", weibull_min.pdf(x + 5, c=2))\n",
    "]\n",
    "\n",
    "plot_all(unimodal, \"Unimodal Distributions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Multimodal distributions ===\n",
    "multimodal = [\n",
    "    (\"Mixture of 2 Gaussians (equal)\", 0.5 * norm.pdf(x, -2, 0.5) + 0.5 * norm.pdf(x, 2, 0.5)),\n",
    "    (\"Mixture of 2 Gaussians (unequal)\", 0.3 * norm.pdf(x, -2, 0.5) + 0.7 * norm.pdf(x, 2, 1)),\n",
    "    (\"Mixture of 3 Gaussians\", 0.2 * norm.pdf(x, -3, 0.7) + 0.5 * norm.pdf(x, 1, 0.5) + 0.3 * norm.pdf(x, 2, 0.3)),\n",
    "    (\"Mixture of 4 Gaussians\", sum([0.25 * norm.pdf(x, mu, 0.3) for mu in [-3, -1, 1, 3]])),\n",
    "    (\"W-shaped Beta (2,5)+(5,2)\", 0.5 * beta.pdf((x + 3)/6, 2, 5) * (1/6) + 0.5 * beta.pdf((x + 3)/6, 5, 2) * (1/6)),\n",
    "    (\"Mixture with far peaks\", 0.5 * norm.pdf(x, -4, 0.5) + 0.5 * norm.pdf(x, 4, 0.5)),\n",
    "    (\"Trimodal mixed shapes\", 0.4 * norm.pdf(x, -3, 0.6) + 0.4 * laplace.pdf(x, 0, 0.5) + 0.2 * norm.pdf(x, 3, 1)),\n",
    "    (\"Beta(0.5, 0.5) scaled\", beta.pdf((x + 3)/6, 0.5, 0.5) * (1/6)),\n",
    "    (\"Mixture of 5 peaks\", sum([0.2 * norm.pdf(x, mu, 0.25) for mu in [-4, -2, 0, 2, 4]])),\n",
    "]\n",
    "\n",
    "plot_all(multimodal, \"Multimodal Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Visualization\n",
    "\n",
    "Use the sliders and checkboxes to visualize the robot (`s`) and the chosen action (`x`).  \n",
    "- Adjust the initial position (`s`) of the robot.\n",
    "- Toggle visibility of the obstacle, robot, and action squares.\n",
    "- Observe collision detection status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an interactive visualization of the problem\n",
    "\n",
    "# --- Configuration ---\n",
    "SQUARE_SIZE = 1.0\n",
    "OBSTACLE_POS = 0.0\n",
    "\n",
    "# Y positions for elements\n",
    "Y_OBSTACLE = 1\n",
    "Y_ACTION = 1\n",
    "Y_ROBOT = -1\n",
    "PLOT_XLIM = (-2, 2)\n",
    "PLOT_YLIM = (-2, 2)\n",
    "\n",
    "# --- Plotting Functions ---\n",
    "\n",
    "def draw_square(ax, center_x, center_y, color=None, facecolor=None, fill=True, edgecolor='black', label=None):\n",
    "    \"\"\"Draws a square centered at (center_x, center_y).\"\"\"\n",
    "    half_size = SQUARE_SIZE / 2\n",
    "    rect = plt.Rectangle(\n",
    "        (center_x - half_size, center_y - half_size), # Use center_y\n",
    "        SQUARE_SIZE,\n",
    "        SQUARE_SIZE,\n",
    "        facecolor=facecolor if facecolor else color, # Use facecolor if provided\n",
    "        fill=fill,\n",
    "        edgecolor=edgecolor,\n",
    "        linewidth=2 if not fill else 1,\n",
    "        label=label\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "def check_collision(action_x):\n",
    "    \"\"\"Checks if the action collides with the obstacle.\"\"\"\n",
    "    obstacle_left = OBSTACLE_POS - SQUARE_SIZE / 2\n",
    "    obstacle_right = OBSTACLE_POS + SQUARE_SIZE / 2\n",
    "    action_left = action_x - SQUARE_SIZE / 2\n",
    "    action_right = action_x + SQUARE_SIZE / 2\n",
    "\n",
    "    return max(obstacle_left, action_left) < min(obstacle_right, action_right)\n",
    "\n",
    "# --- Interactive Setup ---\n",
    "\n",
    "# Widgets\n",
    "slider_robot_s = widgets.FloatSlider(value=0.0, min=-1.5, max=1.5, step=0.1, description='Robot s:')\n",
    "slider_action_x = widgets.FloatSlider(value=1.05, min=-1.5, max=1.5, step=0.1, description='Action x:')\n",
    "checkbox_show_robot = widgets.Checkbox(value=True, description='Show Robot (s)')\n",
    "checkbox_show_obstacle = widgets.Checkbox(value=False, description='Show Obstacle')\n",
    "checkbox_show_action = widgets.Checkbox(value=False, description='Show Action (x)')\n",
    "status_label = widgets.Label(value=\"\")\n",
    "\n",
    "# Output widget to hold the plot\n",
    "output_plot = widgets.Output()\n",
    "\n",
    "def update_plot(*args):\n",
    "    \"\"\"Clears and redraws the plot based on widget values.\"\"\"\n",
    "    with output_plot:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "        s = slider_robot_s.value\n",
    "        x_action = slider_action_x.value\n",
    "        show_robot = checkbox_show_robot.value\n",
    "        show_obstacle = checkbox_show_obstacle.value\n",
    "        show_action = checkbox_show_action.value\n",
    "\n",
    "        # Draw elements based on visibility flags and new Y positions\n",
    "        if show_obstacle:\n",
    "            draw_square(ax, OBSTACLE_POS, Y_OBSTACLE, color='red', fill=True, label='Obstacle')\n",
    "        if show_robot:\n",
    "            draw_square(ax, s, Y_ROBOT, color='green', fill=True, label='Robot (s)')\n",
    "\n",
    "            # Calculate arrow start point (top-middle of robot)\n",
    "            arrow_start_x = s\n",
    "            arrow_start_y = Y_ROBOT + SQUARE_SIZE / 2\n",
    "\n",
    "            # Determine arrow endpoint and calculate dx, dy\n",
    "            if show_action:\n",
    "                # Point to bottom-middle of action square\n",
    "                arrow_end_x = x_action\n",
    "                arrow_end_y = Y_ACTION - SQUARE_SIZE / 2\n",
    "                arrow_dx = arrow_end_x - arrow_start_x\n",
    "                arrow_dy = arrow_end_y - arrow_start_y\n",
    "            else:\n",
    "                # Point straight up with length 1\n",
    "                arrow_dx = 0\n",
    "                arrow_dy = 1.0 # Fixed length when action is hidden\n",
    "\n",
    "            # Draw the arrow\n",
    "            ax.arrow(arrow_start_x, arrow_start_y, arrow_dx, arrow_dy,\n",
    "                     head_width=0.1, head_length=0.15, fc='green', ec='green', length_includes_head=True)\n",
    "\n",
    "        if show_action:\n",
    "            # Use facecolor='none' for outline, specify edgecolor explicitly\n",
    "            draw_square(ax, x_action, Y_ACTION, facecolor='none', fill=False, edgecolor='green', label='Action (x)')\n",
    "\n",
    "            # Check collision and update status ONLY if action is shown\n",
    "            is_collision = check_collision(x_action)\n",
    "            status = \"Fail!\" if is_collision else \"Success!\"\n",
    "            status_label.value = f\"Status: {status}\"\n",
    "            ax.text(0, PLOT_YLIM[1] * 0.9, status, ha='center', va='top', fontsize=12, color='red' if is_collision else 'darkgreen')\n",
    "        else:\n",
    "             status_label.value = \"\"\n",
    "\n",
    "\n",
    "        # Plot Formatting\n",
    "        ax.set_xlim(PLOT_XLIM)\n",
    "        ax.set_ylim(PLOT_YLIM)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel(\"Horizontal Position\")\n",
    "        ax.set_title(\"Interactive Environment Demo\")\n",
    "        # Add horizontal lines for reference\n",
    "        ax.axhline(Y_OBSTACLE, color='grey', linestyle='--', linewidth=0.5)\n",
    "        ax.axhline(Y_ROBOT, color='grey', linestyle='--', linewidth=0.5)\n",
    "        ax.grid(True, axis='x', linestyle=':', linewidth=0.5)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Observe changes in widgets and call update_plot\n",
    "slider_robot_s.observe(update_plot, names='value')\n",
    "slider_action_x.observe(update_plot, names='value')\n",
    "checkbox_show_robot.observe(update_plot, names='value')\n",
    "checkbox_show_obstacle.observe(update_plot, names='value')\n",
    "checkbox_show_action.observe(update_plot, names='value')\n",
    "\n",
    "# Arrange widgets vertically\n",
    "controls = widgets.VBox([\n",
    "    slider_robot_s,\n",
    "    slider_action_x,\n",
    "    checkbox_show_robot,\n",
    "    checkbox_show_obstacle,\n",
    "    checkbox_show_action,\n",
    "    status_label\n",
    "])\n",
    "\n",
    "# Arrange plot and controls horizontally\n",
    "app_layout = widgets.HBox([output_plot, controls])\n",
    "\n",
    "# Display the combined layout\n",
    "display(app_layout)\n",
    "\n",
    "# Initial plot draw\n",
    "update_plot()\n",
    "\n",
    "print(\"Interactive demo created. Run this script in a Jupyter Notebook or similar environment.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets\n",
    "\n",
    "Generate the clean expert dataset and a noisy version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expert_data(num_samples=NUM_EXPERT_SAMPLES,\n",
    "                         data_params=DEFAULT_DATA_PARAMS):\n",
    "    \"\"\"\n",
    "    Generates simulated expert demonstration data for the 1D jumper scenario.\n",
    "    Uses parameters from config by default.\n",
    "    \"\"\"\n",
    "    p = data_params\n",
    "    s_min, s_max = p['s_min'], p['s_max']\n",
    "    safe_boundary = p['safe_boundary']\n",
    "    target_mean_delta = p['target_mean_delta']\n",
    "    lognorm_sigma = p['lognorm_sigma']\n",
    "\n",
    "    s = np.random.uniform(s_min, s_max, num_samples)\n",
    "    prob_right = (s + 1) / 2\n",
    "    prob_right = np.clip(prob_right, 0.0, 1.0)\n",
    "    jump_right = np.random.rand(num_samples) < prob_right\n",
    "\n",
    "    lognorm_mu = np.log(target_mean_delta) - (lognorm_sigma**2 / 2)\n",
    "    delta = lognorm.rvs(s=lognorm_sigma, scale=np.exp(lognorm_mu), size=num_samples)\n",
    "\n",
    "    x = np.zeros(num_samples)\n",
    "    x[jump_right] = safe_boundary + delta[jump_right]\n",
    "    x[~jump_right] = -safe_boundary - delta[~jump_right]\n",
    "\n",
    "    # Clip x values between -3 and 3\n",
    "    x = np.clip(x, -3*safe_boundary, 3*safe_boundary)\n",
    "\n",
    "    df = pd.DataFrame({'s': s, 'x': x})\n",
    "    return df\n",
    "\n",
    "\n",
    "expert_data = generate_expert_data(num_samples=NUM_EXPERT_SAMPLES)\n",
    "print(f\"Generated {len(expert_data)} expert samples for training.\")\n",
    "\n",
    "test_data = generate_expert_data(num_samples=N_TEST_SAMPLES)\n",
    "print(f\"Generated {len(test_data)} samples for testing/plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(test_data=expert_data, title=\"Training Data Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(test_data=test_data, title=\"Test Data Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MSE regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(expert_data[['s']].values, dtype=torch.float32).to(DEVICE)\n",
    "y_train = torch.tensor(expert_data[['x']].values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model_mse = MLP(input_dim=1, output_dim=1).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model_mse.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(f\"Training model for {EPOCHS} epochs...\")\n",
    "for _ in range(EPOCHS):\n",
    "    model_mse = train_one_epoch(model_mse, train_loader, optimizer, loss_fn)\n",
    "\n",
    "model_mse.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_s_tensor = torch.tensor(test_data[['s']].values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_pred_tensor = model_mse(test_s_tensor)\n",
    "x_pred_np = x_pred_tensor.cpu().numpy().flatten()\n",
    "\n",
    "x_pred_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Success rate (beyond {SAFE_BOUNDARY} boundary): {(np.abs(x_pred_np) > SAFE_BOUNDARY).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    test_data['s'], x_pred_np, test_data=test_data, \n",
    "    title=\"BC (MSE) Predictions vs. Test Data\",\n",
    "    model_label=\"MLP BC (MSE)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a set of numbers $x_1, x_2, \\dots, x_N$ and want to find the prediction $p$ that minimizes the Mean Squared Error (MSE), we define the loss as:\n",
    "\n",
    "$$\n",
    "\\text{MSE}(p) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - p)^2\n",
    "$$\n",
    "\n",
    "The minimum is achieved when $p$ equals the **mean** of the values:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{N} \\sum_{i=1}^{N} x_i = \\text{mean}(x)\n",
    "$$\n",
    "\n",
    "So, the best constant prediction under MSE is simply the average of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data = expert_data[(expert_data['s'] > 0.6) & (expert_data['s'] < 0.7)]\n",
    "\n",
    "print(f\"Mean: {demo_data['x'].mean()}\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(demo_data['x'], bins=100, density=True, alpha=0.7)\n",
    "plt.axvline(demo_data['x'].mean(), color='g', linestyle='--', label=f'Mean={demo_data[\"x\"].mean():.2f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of x values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data = expert_data#[(expert_data['s'] > 0.6) & (expert_data['s'] < 0.7)]\n",
    "\n",
    "print(f\"Mean: {demo_data['x'].mean()}\")\n",
    "print(f\"Median: {demo_data['x'].median()}\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(demo_data['x'], bins=100, density=True, alpha=0.7)\n",
    "plt.axvline(demo_data['x'].mean(), color='g', linestyle='--', label=f'Mean={demo_data[\"x\"].mean():.2f}')\n",
    "plt.axvline(demo_data['x'].median(), color='r', linestyle='--', label=f'Median={demo_data[\"x\"].median():.2f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of x values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to minimize the Mean Absolute Error (MAE) over a set of values $x_1, x_2, \\dots, x_N$, we define the loss as:\n",
    "\n",
    "$$\n",
    "\\text{MAE}(p) = \\frac{1}{N} \\sum_{i=1}^{N} |x_i - p|\n",
    "$$\n",
    "\n",
    "Unlike MSE, this loss is minimized not by the mean, but by the **median** of the values:\n",
    "\n",
    "$$\n",
    "p = \\text{median}(x)\n",
    "$$\n",
    "\n",
    "So, under MAE, the best constant prediction is the median of the data — which makes it more robust to outliers compared to MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mae = MLP(input_dim=1, output_dim=1).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model_mae.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "print(f\"Training model for {EPOCHS} epochs...\")\n",
    "for _ in range(EPOCHS):\n",
    "    model_mae = train_one_epoch(model_mae, train_loader, optimizer, loss_fn)\n",
    "\n",
    "model_mae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_pred_tensor = model_mae(test_s_tensor)\n",
    "x_pred_np = x_pred_tensor.cpu().numpy().flatten()\n",
    "\n",
    "x_pred_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Success rate (beyond {SAFE_BOUNDARY} boundary): {(np.abs(x_pred_np) > SAFE_BOUNDARY).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    test_data['s'], x_pred_np, test_data=test_data, \n",
    "    title=\"MAE trained model Predictions vs. Test Data\",\n",
    "    model_label=\"MLP BC (MAE)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use even better loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 6 functions that can be used as a loss function\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "\n",
    "functions = [\n",
    "    (r\"$x^2$\", lambda x: x**2),\n",
    "    (r\"$|x|$\", lambda x: np.abs(x)),\n",
    "    (r\"$1 - e^{-x^2}$\", lambda x: 1 - np.exp(-x**2)),\n",
    "    (r\"$\\min(|x|, 2)$\", lambda x: np.minimum(np.abs(x), 2)),\n",
    "    (r\"$\\log(1 + x^2)$\", lambda x: np.log1p(x**2)),\n",
    "    (r\"$1 - e^{-|x|}$\", lambda x: 1 - np.exp(-np.abs(x))),\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "for ax, (title, func) in zip(axs.flat, functions):\n",
    "    ax.plot(x, func(x), lw=4, color='orange')\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target tokenization and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clf = MLP(input_dim=1, output_dim=1).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model_clf.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "y_train = torch.tensor(expert_data[['x']].values > 0, dtype=torch.float32).to(DEVICE)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Training model for {EPOCHS} epochs...\")\n",
    "for _ in range(EPOCHS):\n",
    "    model_clf = train_one_epoch(model_clf, train_loader, optimizer, loss_fn)\n",
    "\n",
    "model_clf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_left = expert_data[expert_data['x'] <= 0]['x'].mean()\n",
    "mean_right = expert_data[expert_data['x'] > 0]['x'].mean()\n",
    "print(f\"Mean left: {mean_left}, Mean right: {mean_right}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_cls_pred = model_clf(test_s_tensor)\n",
    "    x_pred_tensor = (x_cls_pred > 0) * mean_right + (x_cls_pred <= 0) * mean_left\n",
    "    \n",
    "x_pred_np = x_pred_tensor.cpu().numpy().flatten()\n",
    "\n",
    "x_pred_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Success rate (beyond {SAFE_BOUNDARY} boundary): {(np.abs(x_pred_np) > SAFE_BOUNDARY).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    test_data['s'], x_pred_np, test_data=test_data, \n",
    "    title=\"Classification model Predictions vs. Test Data\",\n",
    "    model_label=\"Classification\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_cls_pred = torch.sigmoid(model_clf(test_s_tensor))\n",
    "    probs = torch.rand_like(x_cls_pred)\n",
    "    x_pred_tensor = (probs < x_cls_pred) * mean_right + (probs >= x_cls_pred) * mean_left\n",
    "    \n",
    "x_pred_np = x_pred_tensor.cpu().numpy().flatten()\n",
    "\n",
    "x_pred_np[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Random Sampling\n",
    "\n",
    "To introduce action diversity, instead of always taking the class with the highest probability, we sample the action according to the predicted probability distribution.  \n",
    "This can be useful if you want the robot to occasionally pick less likely but still valid actions, improving behavior diversity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    test_data['s'], x_pred_np, test_data=test_data, \n",
    "    title=\"Classification model with random sampling vs. Test Data\",\n",
    "    model_label=\"Classification\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_steps = get_noisy_steps(\"media/robot1.png\", 4)\n",
    "plot_noisy_steps(noisy_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_steps = get_noisy_steps(\"media/robot2.png\", 4)\n",
    "plot_noisy_steps(noisy_steps, True, \"Denoising diffusion model inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_np = noisy_steps[0]\n",
    "noise_img = noisy_steps[-1]\n",
    "\n",
    "# Plot side-by-side: noise -> image\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(noise_img)\n",
    "axes[0].set_title(\"Random noise\", fontsize=18)\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(original_np)\n",
    "axes[1].set_title(\"Reasonable image\", fontsize=18)\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Add arrow\n",
    "fig.text(0.5, 0.5, '→', fontsize=30, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "expert_data_x = expert_data['x'].values\n",
    "\n",
    "# Plot both distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "# Left: standard normal\n",
    "sns.kdeplot(np.random.randn(100000), fill=True, color='blue', ax=axes[0], label='Random Noise')\n",
    "axes[0].set_title(\"Random noise\", fontsize=18)\n",
    "axes[0].set_xlabel(\"Action (x)\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: expert data\n",
    "sns.kdeplot(expert_data['x'], fill=True, color='blue', ax=axes[1], label='Data Distribution', bw_adjust=0.05)\n",
    "axes[1].set_title(\"Reasonable actions\", fontsize=18)\n",
    "axes[1].set_xlabel(\"Action (x)\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Add arrow between plots\n",
    "fig.text(0.53, 0.5, '→', fontsize=30, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward process we are adding noise to the data at each step.\n",
    "\n",
    "There are different ways to add noise. For simplicity, I will use this example:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{a_t} \\, x_{t-1} + \\sqrt{1 - a_t} \\, \\varepsilon\n",
    "$$\n",
    "\n",
    "where $a_t$ is predefined by us, and $\\varepsilon$ is sampled from a normal distribution with mean 0 and standard deviation equal to the data std.\n",
    "\n",
    "\n",
    "\n",
    "In practice, instead of modeling it step by step, we model how the noisy signal looks after $t$ steps:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{a}_t} \\, x_0 + \\sqrt{1 - \\bar{a}_t} \\, \\varepsilon\n",
    "$$\n",
    "\n",
    "where $\\bar{a}_t$ is the cumulative product of $a_t$.\n",
    "\n",
    "$$\n",
    "\\bar{a}_t = \\prod_{s=1}^{t} a_s\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise scheduling is very important for the model to work well.\n",
    "\n",
    "T = 49 # Number of diffusion steps\n",
    "\n",
    "# Noise variance at each step\n",
    "b_t = torch.linspace(0.0001, 0.2, T) # I use pretty high noise as we have a very low number of steps\n",
    "\n",
    "# Per steps signal retention coefficients \n",
    "a_t = 1 - b_t\n",
    "\n",
    "# Cumulative product of signal retention coefficients\n",
    "a_cumprod_t = torch.cumprod(a_t, dim=0)\n",
    "\n",
    "print(a_t)\n",
    "print(a_cumprod_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_diffusion = MLP(input_dim=3, output_dim=1).to(DEVICE)\n",
    "optimizer = optim.Adam(model_diffusion.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "x_tensor = torch.tensor(expert_data['x'], device=DEVICE)\n",
    "s_tensor = torch.tensor(expert_data['s'], device=DEVICE)\n",
    "\n",
    "print(f\"Training model for {EPOCHS} epochs...\")\n",
    "for _ in range(EPOCHS):\n",
    "    # For transparency I will regenerate the data at each epoch here so it is easier to follow the code\n",
    "\n",
    "    # Generate a random step from 1 to T for each sample\n",
    "    # We will use only one step per sample per epoch for simplicity\n",
    "    t = torch.randint(0, T, (len(expert_data),), device=DEVICE)\n",
    "\n",
    "    # Generate extra noise\n",
    "    e = torch.randn(len(expert_data), device=DEVICE)\n",
    "\n",
    "    # Generate x_t\n",
    "    x_t = torch.sqrt(a_cumprod_t[t]) * x_tensor + torch.sqrt(1 - a_cumprod_t[t]) * e\n",
    "    \n",
    "    # The input for the model will be s, t, and x_t\n",
    "    X_train = torch.stack([s_tensor, t.float()/T, x_t], dim=1).float()\n",
    "\n",
    "    # And the target will be e\n",
    "    y_train = e.unsqueeze(-1)\n",
    "\n",
    "    # Create a dataset and a dataloader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Now let't train 1 bath\n",
    "    model_diffusion = train_one_epoch(model_diffusion, train_loader, optimizer, loss_fn)\n",
    "\n",
    "model_diffusion.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we have a trained model, we can sample random noise from the normal distribution and restore it by iteratively applying the model $T$ times.  \n",
    "Each time we use the model to predict the noise and subtract it from the current state:\n",
    "\n",
    "$$\n",
    "e_t = \\text{model\\_diffusion}(s, t, x_t)\n",
    "$$\n",
    "\n",
    "There are different ways to reconstruct the action using the model prediction.  \n",
    "I will show the one with the simplest math: DDIM with $\\eta = 0$.\n",
    "\n",
    "Let's look at the formula for the forward process:\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{a}_t} \\, x_0 + \\sqrt{1 - \\bar{a}_t} \\, \\varepsilon \\tag{1}\n",
    "$$\n",
    "\n",
    "For $x_{t-1}$, we can rewrite (1) as:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\sqrt{\\bar{a}_{t-1}} \\, x_0 + \\sqrt{1 - \\bar{a}_{t-1}} \\, \\varepsilon \\tag{2}\n",
    "$$\n",
    "\n",
    "Now let's estimate $x_0$ from (1):\n",
    "\n",
    "$$\n",
    "x_0 = \\frac{1}{\\sqrt{\\bar{a}_t}} \\left( x_t - \\sqrt{1 - \\bar{a}_t} \\, \\varepsilon \\right) \\tag{3}\n",
    "$$\n",
    "\n",
    "Now we can plug (3) into (2), assuming that $\\varepsilon$ is the same for both equations:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\sqrt{\\bar{a}_{t-1}} \\, \\frac{1}{\\sqrt{\\bar{a}_t}} \\left( x_t - \\sqrt{1 - \\bar{a}_t} \\, \\varepsilon \\right) + \\sqrt{1 - \\bar{a}_{t-1}} \\, \\varepsilon\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "- $\\bar{a}_{t-1} / \\bar{a}_t = 1/a_t$\n",
    "- Rearranging terms, we get:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\frac{1}{\\sqrt{a_t}} \\left( x_t - \\left( \\sqrt{1 - \\bar{a}_t} - \\sqrt{a_t - \\bar{a}_t} \\right) \\varepsilon \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "noise = np.random.randn(len(test_s_tensor))\n",
    "predictions.append(noise)\n",
    "x_t_tensor = torch.tensor(noise, dtype=torch.float32).to(DEVICE).unsqueeze(-1)\n",
    "\n",
    "for t in range(T-1, -1, -1):\n",
    "    t_tensor = torch.ones_like(test_s_tensor) * t/T\n",
    "    X_train = torch.cat([test_s_tensor, t_tensor, x_t_tensor], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        e_pred = model_diffusion(X_train)\n",
    "\n",
    "    x_t_tensor = (1/ a_t[t]) ** 0.5 * (x_t_tensor - ((1 - a_cumprod_t[t])** 0.5 - (a_t[t] - a_cumprod_t[t]) ** 0.5 ) * e_pred)\n",
    "\n",
    "    predictions.append(x_t_tensor.cpu().numpy().flatten())\n",
    "\n",
    "x_pred_np = np.array(predictions[-1])\n",
    "x_pred_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Success rate (beyond {SAFE_BOUNDARY} boundary): {(np.abs(x_pred_np) > SAFE_BOUNDARY).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    test_data['s'], x_pred_np, test_data=test_data, \n",
    "    title=\"Diffusion model Predictions vs. Test Data\",\n",
    "    model_label=\"Diffusion model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution_distributions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Matching Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw an intuitive (but not strictly correct) analogy between Diffusion and Flow Matching using the relationship between summation and integration:\n",
    "\n",
    "$$\n",
    "\\sum_{n=1}^{N} f\\left(\\frac{n}{N}\\right) \\cdot \\frac{1}{N} \\approx \\int_{0}^{1} f(x) \\, dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "Diffusion ≈ Flow Matching\n",
    "$$\n",
    "This is just a conceptual analogy:  \n",
    "- Diffusion discretizes time into fixed steps and predicts denoising step-by-step.  \n",
    "- Flow Matching treats time as continuous and directly learns the vector field.\n",
    "\n",
    "So, just like Riemann sums approximate integrals, Flow Matching can be seen as a continuous-time simplification of Diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "\n",
    "# Load and preprocess image\n",
    "img_path = \"media/robot2.png\"\n",
    "x0_img = Image.open(img_path).convert(\"RGB\").resize((128, 128))\n",
    "x0_np = np.array(x0_img).astype(np.float32) / 255.0\n",
    "\n",
    "# Generate random noise with fixed seed\n",
    "np.random.seed(42)\n",
    "e_np = np.random.randn(*x0_np.shape).astype(np.float32)\n",
    "\n",
    "# Create animation\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "img_display = ax.imshow(np.zeros_like(x0_np), animated=True)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "def update(frame_idx):\n",
    "    t = np.linspace(0, 1, 50)[frame_idx]\n",
    "    x_t = t * x0_np + (1 - t) * e_np  # Simplified interpolation\n",
    "    x_t = np.clip(x_t, 0, 1)\n",
    "    img_display.set_array(x_t)\n",
    "    return [img_display]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=50, interval=50, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow Matching feels like Diffusion stripped of a lot of complexity.\n",
    "\n",
    "We don't need $a_t$ and $b_t$ anymore.\n",
    "\n",
    "During training, for each sample we just sample two random numbers:\n",
    "- $\\varepsilon$ — noise from a normal distribution\n",
    "- $t$ — time from $0$ to $1$ (uniform distribution)\n",
    "\n",
    "Then we use a simple linear interpolation between the actual $x$ and the noise $\\varepsilon$ to get the noisy input $x_t$:\n",
    "\n",
    "$$\n",
    "x_t = x \\cdot t + \\varepsilon \\cdot (1 - t)\n",
    "$$\n",
    "\n",
    "Finally, similar to the Diffusion model, we use $s$, $t$, and $x_t$ as input to the model, but the training target is simply:\n",
    "\n",
    "$$\n",
    "model(s, t, x_t) \\approx x - \\varepsilon\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_flow_matching = MLP(input_dim=3, output_dim=1).to(DEVICE)\n",
    "optimizer = optim.Adam(model_flow_matching.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "x_tensor = torch.tensor(expert_data['x'], dtype=torch.float32, device=DEVICE)\n",
    "s_tensor = torch.tensor(expert_data['s'], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "print(f\"Training model for {EPOCHS} epochs...\")\n",
    "for _ in range(EPOCHS):\n",
    "    # For transparency I will regenerate the data at each epoch here so it is easier to follow the code\n",
    "\n",
    "    # Generate a random time from 0 to 1 for each sample\n",
    "    # We will use only one step per sample per epoch for simplicity\n",
    "    t = torch.rand(len(expert_data), dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # Generate noise\n",
    "    e = torch.randn(len(expert_data), dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # Generate x_t\n",
    "    x_t = x_tensor * t + e * (1 - t)\n",
    "\n",
    "    # The input for the model will be s, t, and x_t\n",
    "    X_train = torch.stack([s_tensor,t,x_t], dim=1).float()\n",
    "\n",
    "    # And the target will be e\n",
    "    y_train = (x_tensor - e).unsqueeze(-1)\n",
    "\n",
    "    # Create a dataset and a dataloader\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Now let't train 1 bath\n",
    "    model_flow_matching = train_one_epoch(model_flow_matching, train_loader, optimizer, loss_fn)\n",
    "\n",
    "model_flow_matching.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction is also straightforward in case of Flow Matching:\n",
    "\n",
    "$$\n",
    "x_{t+\\tau} = x_t + \\tau \\cdot \\text{model\\_flow\\_matching}(s, t, x_{t})\n",
    "$$\n",
    "\n",
    "Where $\\tau$ is a small step that we can select based on the accuracy / speed trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "noise = np.random.randn(len(test_s_tensor))\n",
    "predictions.append(noise)\n",
    "x_t_tensor = torch.tensor(noise, dtype=torch.float32).to(DEVICE).unsqueeze(-1)\n",
    "\n",
    "T = 49\n",
    "for t in range(0, T):\n",
    "    t_tensor = torch.ones_like(test_s_tensor) * t/T\n",
    "    X_train = torch.cat([test_s_tensor, t_tensor, x_t_tensor], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        e_pred = model_flow_matching(X_train)\n",
    "\n",
    "    x_t_tensor = x_t_tensor + e_pred * 1/T\n",
    "\n",
    "    predictions.append(x_t_tensor.cpu().numpy().flatten())\n",
    "\n",
    "x_pred_np = np.array(predictions[-1])\n",
    "x_pred_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Success rate (beyond {SAFE_BOUNDARY} boundary): {(np.abs(x_pred_np) > SAFE_BOUNDARY).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(\n",
    "    test_data['s'], x_pred_np, test_data=test_data, \n",
    "    title=\"Flow matching model Predictions vs. Test Data\",\n",
    "    model_label=\"Flow matching model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution_distributions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- ACT: Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware  \n",
    "  https://arxiv.org/abs/2304.13705\n",
    "\n",
    "- Diffusion Policy: Visuomotor Policy Learning via Action Diffusion  \n",
    "  https://arxiv.org/abs/2303.04137\n",
    "\n",
    "- Behavior Transformers: Cloning k Modes with One Stone  \n",
    "  https://arxiv.org/abs/2206.11251\n",
    "\n",
    "- VQ-BET: Behavior Generation with Latent Actions  \n",
    "  https://arxiv.org/abs/2403.03181\n",
    "\n",
    "- Denoising Diffusion Probabilistic Models  \n",
    "  https://arxiv.org/abs/2006.11239\n",
    "\n",
    "- DDIM: Denoising Diffusion Implicit Models  \n",
    "  https://arxiv.org/abs/2010.02502\n",
    "\n",
    "- Flow Matching for Generative Modeling  \n",
    "  https://arxiv.org/abs/2210.02747\n",
    "\n",
    "- MIT Course on Diffusion & Flow Matching (SDE perspective)  \n",
    "  https://diffusion.csail.mit.edu/\n",
    "\n",
    "- My DOT-Policy repo  \n",
    "  https://github.com/IliaLarchenko/dot_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
